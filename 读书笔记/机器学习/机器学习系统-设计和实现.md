# 机器学习系统：设计和实现

## 11. 分布式训练

### 11.6 参数服务器

- 一个核心作用是可以处理分布式训练服务器中出现的落后者

#### 11.6.1 系统架构

在训练的过程中，每个训练服务器都会有完整的模型，根据本地的训练数据集切片（Dataset Shard）训练出梯度。这个梯度会被推送（Push）到各自参数服务器。参数服务器等到两个训练服务器都完成梯度推送，开始计算平均梯度，更新参数。它们然后通知训练服务器来拉取（Pull）最新的参数，开始下一轮训练迭代

#### 11.6.2 异步训练

无须等待同伴，可能会伤害模型的精度，但它同时让不同训练服务器可以按照各自的运算速度推送和拉取参数，避免了落后者对于整个集群性能的影响

- 同步训练： 训练服务器都需要计算平均梯度对每一个模型副本进行更新，从而保证下一轮训练开始前，全部模型副本参数的一致性，这种对于参数一致性的确保一般被称为同步训练
- 同步训练一般有助于训练系统达到更好的模型精度，但是当系统规模变大，往往会观察到落后者服务器的出现，落后者对于基于AllReduce的同步训练系统的性能有显著影响，这是因为AllReduce让全部节点参与到平均梯度的计算和通信中

解决办法：

- 训练服务器训练出梯度后，会把本地梯度全部推送到参数服务器。参数服务器在等到一定训练服务器（例如90%的训练服务器）的梯度后，就开始计算平均梯度
- 利用参数服务器实现异步训练：
  - worker一开始有相同的模型参数副本
  - 计算出梯度后会马上将梯度推送到参数服务器，参数服务器将推送的梯度立刻用于更新参数
  - PS通知训练服务器来拉取最新的参数，从而不同的训练服务器很可能会使用不同版本的模型参数进行梯度计算；

#### 11.6.3 数据副本

解决参数服务器故障和数据热点问题的常用技术是构建模型主从复制

- 增加系统的鲁棒性：当一个参数服务器故障时，系统可以将参数请求导向其他副本
